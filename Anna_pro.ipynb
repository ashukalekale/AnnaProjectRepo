{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wrzl0I7imS63"
      },
      "outputs": [],
      "source": [
        "# 3. Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFYMrZ4ViPE_",
        "outputId": "204d8128-16bb-4885-fc47-0adb695941e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NoM_aM6ashks"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0FfbYx5mTwL"
      },
      "outputs": [],
      "source": [
        "# 4. Set paths (change this)\n",
        "DATA_DIR = \"/content/drive/MyDrive/24NovDATASET/dataset\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRp3R9e8sc7l",
        "outputId": "bcf1242a-a626-4bc2-d17a-fc2491fa3e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: ['watermelon', 'banana', 'Mango', 'cotton']\n"
          ]
        }
      ],
      "source": [
        "CLASSES_DIR = DATA_DIR\n",
        "classes = os.listdir(CLASSES_DIR)\n",
        "print(\"Classes found:\", classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBZWMNbyszlv",
        "outputId": "cdbd58f2-463f-41f4-8b8b-9e3a989044f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution: {'watermelon': 4, 'banana': 2, 'Mango': 8, 'cotton': 4}\n"
          ]
        }
      ],
      "source": [
        "class_counts = {}\n",
        "for c in classes:\n",
        "    class_counts[c] = len(os.listdir(os.path.join(CLASSES_DIR, c)))\n",
        "print(\"Class Distribution:\", class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJOoDvArxy_X",
        "outputId": "ebfe53b8-ddc1-4d40-dbf2-37be949d5ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7522 images belonging to 4 classes.\n",
            "Found 1879 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    classes=classes # Explicitly provide the class names\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    classes=classes # Explicitly provide the class names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UwaoUPnnzIKl"
      },
      "outputs": [],
      "source": [
        "# 6. Build Model with MobileNetV2\n",
        "base_model = MobileNetV2(input_shape=IMAGE_SIZE + (3,), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Explicitly build the model to create variables before compiling\n",
        "model.build(input_shape=(None,) + IMAGE_SIZE + (3,))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S86pF5kZ1xMm",
        "outputId": "b289649b-3822-4899-8648-16d2032a605a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4524s\u001b[0m 19s/step - accuracy: 0.9420 - loss: 0.1673 - val_accuracy: 0.9116 - val_loss: 0.7925\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/235\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:14\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0032"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 578ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9127 - val_loss: 0.8020\n",
            "Epoch 3/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 3s/step - accuracy: 0.9972 - loss: 0.0101 - val_accuracy: 0.9149 - val_loss: 0.6342\n",
            "Epoch 4/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 525ms/step - accuracy: 1.0000 - loss: 0.0211 - val_accuracy: 0.9138 - val_loss: 0.6487\n",
            "Epoch 5/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 3s/step - accuracy: 0.9980 - loss: 0.0102 - val_accuracy: 0.9052 - val_loss: 0.7264\n",
            "Epoch 6/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 561ms/step - accuracy: 1.0000 - loss: 1.5370e-04 - val_accuracy: 0.9079 - val_loss: 0.7395\n",
            "Epoch 7/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 3s/step - accuracy: 0.9976 - loss: 0.0061 - val_accuracy: 0.9138 - val_loss: 0.9224\n",
            "Epoch 8/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 561ms/step - accuracy: 1.0000 - loss: 3.2711e-04 - val_accuracy: 0.9143 - val_loss: 0.9206\n",
            "Epoch 9/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 3s/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9106 - val_loss: 0.8671\n",
            "Epoch 10/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 548ms/step - accuracy: 1.0000 - loss: 4.0446e-04 - val_accuracy: 0.9041 - val_loss: 0.9200\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE, # Ensure consistent steps\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE, # Ensure consistent steps\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"image_classification_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqxnqtLzCd5N",
        "outputId": "9d6c1968-0af6-4dab-bdbc-372ca2aee30b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"image_classification_model.keras\")\n"
      ],
      "metadata": {
        "id": "W6J-9E-iCe9n"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TtCqHyQDAdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}